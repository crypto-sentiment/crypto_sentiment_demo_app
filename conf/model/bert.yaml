path_to_model: trained_models/bert.ckpt

epochs: 3
train_batch_size: 8
val_batch_size: 16
seed: 42
device: cpu

model_name: &model_name distilbert-base-uncased

tokenizer:
    class: transformers.DistilBertTokenizerFast
    params:
        pretrained_model_name_or_path: *model_name

model:
    class: transformers.DistilBertForSequenceClassification
    params:
        pretrained_model_name_or_path: *model_name
        num_labels: 3

optimizer:
    class: transformers.AdamW
    params:
        lr: 0.00005
        weight_decay: 0.001

scheduler:
    params:
        name: linear
        num_warmup_steps: 500