name: bert
version: "latest"

path_to_model: static/models/prod/bert.onnx
checkpoint_path: static/models/dev/bert.ckpt

epochs: 3
train_batch_size: 8
val_batch_size: 16
seed: 42
device: cuda
num_workers: 4

model_name: &model_name distilbert-base-uncased

tokenizer:
    class: transformers.DistilBertTokenizerFast
    params:
        pretrained_model_name_or_path: *model_name
    call_params:
        truncation: True
        padding: True
        return_tensors: pt

model:
    class: transformers.DistilBertForSequenceClassification
    params:
        pretrained_model_name_or_path: *model_name
        num_labels: 3

optimizer:
    class: transformers.AdamW
    params:
        lr: 0.00005
        weight_decay: 0.001

scheduler:
    params:
        name: linear
        num_warmup_steps: 500

onnx_config:
  output_names: ["logits"]
