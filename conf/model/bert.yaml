path_to_model: static/models/bert.ckpt
name: bert
model_version: 0.0.1

epochs: 3
train_batch_size: 8
val_batch_size: 16
seed: 42
device: cpu

model_name: &model_name distilbert-base-uncased

tokenizer:
    class: transformers.DistilBertTokenizerFast
    params:
        pretrained_model_name_or_path: *model_name
    call_params:
        truncation: True
        padding: True

model:
    class: transformers.DistilBertForSequenceClassification
    params:
        pretrained_model_name_or_path: *model_name
        num_labels: 3

optimizer:
    class: transformers.AdamW
    params:
        lr: 0.00005
        weight_decay: 0.001

scheduler:
    params:
        name: linear
        num_warmup_steps: 500